{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0cf44af-6f0d-49e3-92f9-8842646eb2a3",
   "metadata": {},
   "source": [
    "# Example multilabel classification task using a node-embedded graph\n",
    "\n",
    "Most neural network structures cannot natively accept inputs encoded into graph format. The naive approach to embed a graph into tensor space is to apply all node and edge data onto an adjacency matrix. This creates a massive input that will face critical underperformance for large graphs. One solution to this performance problem is treating the node and edge data with separate machine learning models. In simpler cases, where edges don't contain data but only represent spatial relations, we can encode each node into a vector that represents parameters of its spatial position, which massively reduces the size of our feature tensor. \n",
    "\n",
    "This notebook provides an example implementation to classify facebook page categories using the \"karateclub\" facebook dataset. A graph sample of is taken out of the training dataset, and nodes are embedded into vectors spatially, then those vectors are used to train a fully connected softmax classifier using PyTorch. The model is trained on all data not in the sample, and accuracy is plotted on a per sample basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d22036ae-433a-4a3a-ab31-f2fc3ae3a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from karateclub import GraphReader, LabelPropagation, Diff2Vec\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyvis.network import Network\n",
    "from time import time\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "973d3eb1-17c2-45b8-9a62-a6294765a3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is avaliable: True\n",
      "GPU device count: 1\n",
      "GPU device name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Device set to GPU\n"
     ]
    }
   ],
   "source": [
    "def gpu_check():\n",
    "    print(f\"GPU is avaliable: {torch.cuda.is_available()}\")\n",
    "    print(f\"GPU device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(f\"GPU device name: {torch.cuda.get_device_name(current_device)}\")\n",
    "        torch.set_default_device('cuda')\n",
    "        print(\"Device set to GPU\")\n",
    "    else:\n",
    "        print(\"Device default to CPU\") \n",
    "\n",
    "gpu_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f44f85-08f5-49c1-8127-c5102c6f15f8",
   "metadata": {},
   "source": [
    "### Graph Sampling\n",
    "\n",
    "The graph sampling method used in this example is a cluster based sampling. A random node from the graph nodes is selected, as well as all neighbors within distance k. This preserves some of the spatial information of nodes, which leads to higher accuracy for cluster based label prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea1dc3d6-0d06-491d-92bf-e6a49989288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_kc(graph: nx.Graph):\n",
    "    map = {}\n",
    "    \n",
    "    nodes = [i for i in range(len(graph.nodes))]\n",
    "    for index, node in enumerate(graph.nodes.keys()):\n",
    "        map[node] = index\n",
    "    \n",
    "    edges = []\n",
    "    for edge in graph.edges:\n",
    "        mapped_edge = (map[edge[0]], map[edge[1]])\n",
    "        edges.append(mapped_edge)\n",
    "    \n",
    "    mapped_graph = nx.Graph()\n",
    "    mapped_graph.add_nodes_from(nodes)\n",
    "    mapped_graph.add_edges_from(edges)\n",
    "    return mapped_graph, map\n",
    "\n",
    "class KSampler:\n",
    "    def __init__(self, graph: nx.Graph, target, k=2):\n",
    "        self.graph = graph\n",
    "        self.target = target\n",
    "        self.k = k\n",
    "        \n",
    "    def sample(self, start_node, k=None):\n",
    "        if k is None:\n",
    "            k = self.k\n",
    "        \n",
    "        nodes = {start_node}\n",
    "        depth = 0\n",
    "        \n",
    "        while depth < k:\n",
    "            nodes_current = nodes.copy()\n",
    "            for node in nodes_current:\n",
    "                nodes = nodes.union(self.graph.neighbors(node))\n",
    "                                \n",
    "            depth += 1\n",
    "        \n",
    "        nodes = list(nodes)\n",
    "        edges = self.generate_edges(nodes)\n",
    "        return nodes, edges\n",
    "\n",
    "    def generate_edges(self, nodes):\n",
    "        edges = []\n",
    "        n = len(nodes)\n",
    "        if n <= 1:\n",
    "            return\n",
    "        \n",
    "        l = 0\n",
    "        r = 1\n",
    "        while l < n - 1:\n",
    "            edge_data = self.graph.get_edge_data(nodes[l], nodes[r])\n",
    "            if edge_data is not None:\n",
    "                edges.append((nodes[l], nodes[r]))\n",
    "            \n",
    "            r += 1\n",
    "            if r >= n:\n",
    "                l += 1\n",
    "                r = l + 1\n",
    "        \n",
    "        return edges\n",
    "\n",
    "    def sample_as_graph(self, k=None, kc=False):\n",
    "        if k is None:\n",
    "            k = k\n",
    "            \n",
    "        start_node = random.choice(list(self.graph.nodes))\n",
    "        nodes, edges = self.sample(start_node, k)\n",
    "        sample_graph = nx.Graph()\n",
    "        \n",
    "        sample_graph.add_nodes_from(nodes)\n",
    "        sample_graph.add_edges_from(edges)\n",
    "        sample_target = [self.target[i] for i in sample_graph.nodes.keys()]\n",
    "        \n",
    "        if kc:\n",
    "            sample_graph, map = format_kc(sample_graph)\n",
    "            return sample_graph, sample_target, map\n",
    "            \n",
    "        return sample_graph, sample_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a8a3e-fd28-4b1b-9749-4d8042d991b9",
   "metadata": {},
   "source": [
    "### Working with the data\n",
    "\n",
    "First, nodes are converted to vector format using the Diff2Vec model proposed by Rozemberczki and Sarkar, which can be found [here](https://arxiv.org/abs/2001.07463). This method produces a series of \"diffusion-like\" sequences to produce features describing the data related to nearest neighbors; these sequences become iteratively more accurate through a machine learning process. This turns each node in a graph of $m$ nodes into a vector of size $n$, which can be formed into a matrix of size $m \\times n$.\n",
    "\n",
    "Once this is done, the target labels are one-hot encoded and the data is formatted into training and testing feature matrices and target matrices, then put into a form suitable for machine learning with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23998fa7-855a-40f0-8866-a862001e7018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddedDataset(Dataset):\n",
    "    def __init__(self, X, y):        \n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "        \n",
    "        self.features = torch.Tensor(self.features).float().cuda()\n",
    "        self.labels = torch.Tensor(self.labels).float().cuda()\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return self.features[key], self.labels[key]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, reader: GraphReader, embedded_dim=32):\n",
    "        self.graph = reader.get_graph()\n",
    "        self.target = reader.get_target()\n",
    "        \n",
    "        # Node feature embedding\n",
    "        embedder = Diff2Vec(diffusion_number=4, diffusion_cover=30, dimensions=embedded_dim)\n",
    "        embedder.fit(self.graph)\n",
    "        \n",
    "        # One hot encoding\n",
    "        label_count = len(pd.unique(self.target))\n",
    "        labels = np.zeros(shape=(self.target.shape[0], label_count))\n",
    "        for index, label in enumerate(self.target):\n",
    "            labels[index, label] = 1\n",
    "\n",
    "        self.features = np.array(embedder.get_embedding())\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.features[key], self.labels[key]\n",
    "\n",
    "    def split(self, k=2):\n",
    "        self.sampler = KSampler(self.graph, self.target, k=k)\n",
    "        self.sample_graph, self.sample_target = self.sampler.sample_as_graph(kc=False)\n",
    "\n",
    "        graph_node_set = set(list(self.graph.nodes))\n",
    "        sample_node_set = set(list(self.sample_graph.nodes))\n",
    "\n",
    "        train_nodes = np.array(list(graph_node_set.difference(sample_node_set))).astype(int)\n",
    "        test_nodes = np.array(self.sample_graph.nodes).astype(int)\n",
    "        \n",
    "        X_train, X_test = self.features[train_nodes], self.features[test_nodes]\n",
    "        y_train, y_test = self.labels[train_nodes, :], self.labels[test_nodes, :]\n",
    "\n",
    "        train_dataset = EmbeddedDataset(X_train, y_train)\n",
    "        test_dataset = EmbeddedDataset(X_test, y_test)\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b3a9a7e-609d-43b5-9450-37c97ba9e72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding graph and preparing dataset, hold on this can take a while...\n",
      "Dataset done loading!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_features = 64\n",
    "reader = GraphReader('facebook')\n",
    "\n",
    "print(\"Embedding graph and preparing dataset, hold on this can take a while...\")\n",
    "dataset = GraphDataset(reader, embedded_dim=embedding_features)\n",
    "train_dataset, test_dataset = dataset.split()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "print(\"Dataset done loading!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16148b4-fc85-41ea-9a7d-ec5663d4a9df",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "The model used takes in an $m \\times n$ matrix with $n$ embedded features and $m$ nodes, then passes this through a hidden layer with 32 features with the softmax activation function, then finally to the output layer of four features (one for each classification label). The components of this output vector will be between 0 and 1, representing the probability that a node is that label. To measure accuracy, the most likely component is taken to be the predicted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6994227a-c409-4a31-aab9-e1a92cbd61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(embedding_features, 32),\n",
    "    nn.Softmax(dim=1),\n",
    "    nn.Linear(32, 4),\n",
    "    nn.Softmax(dim=1)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e81ef2f-89e4-47e0-a34e-1d394605aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, dataloader: DataLoader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    while True:\n",
    "        for X, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        yield loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea27a1-7ff2-40e5-af76-76347272e263",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "The model learns what embedded features are associated with what labels, and trains over the entire graph (minus the sample) for a number of epochs as a hyperparameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac1101d4-e229-4bfe-9844-94eb927902d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/1\n",
      "Loss at epoch 1: 1.3512802124023438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "print(\"Training model...\")\n",
    "trainer = train(model, train_dataloader)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    last_loss = next(trainer)\n",
    "    print(f\"Loss at epoch {epoch+1}: {last_loss}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de51d0bb-a820-4fc9-b88d-fc2e4319c0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model accuracy on sample...\n",
      "Model has an f1 score of: 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "X_test = test_dataset.features\n",
    "\n",
    "y_pred = np.argmax(model(X_test).cpu().detach(), axis=1)\n",
    "y_test = np.argmax(test_dataset.labels.cpu().detach(), axis=1)\n",
    "\n",
    "y_diff = np.array(y_pred == y_test).astype(np.int32)\n",
    "\n",
    "print(\"Evaluating model accuracy on sample...\")\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "print(f\"Model has an f1 score of: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d492d67-30dd-406f-9160-c9814c429cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_graph(graph, target, name='karateclub', kc=False):\n",
    "    color_dct = {\n",
    "        0:'red',\n",
    "        1:'green',\n",
    "        2:'blue',\n",
    "        3:'yellow'\n",
    "    }\n",
    "    if kc:\n",
    "        graph, mapper = format_kc(graph)\n",
    "    \n",
    "    nodes = graph.nodes.keys()\n",
    "    edges = graph.edges()\n",
    "    \n",
    "    nodes = [int(i) for i in nodes]\n",
    "    colors = [color_dct[target[i]] for i in nodes]\n",
    "    labels = [str(i) for i in nodes]\n",
    "    \n",
    "    net = Network(notebook=True,\n",
    "                  cdn_resources='remote',\n",
    "                  bgcolor='#222222',\n",
    "                  font_color = \"white\",\n",
    "                  height='400px',\n",
    "                  width='400px'\n",
    "                  )    \n",
    "    net.add_nodes(nodes, color=colors, label=labels)\n",
    "    net.add_edges(edges)\n",
    "    \n",
    "    net.force_atlas_2based()\n",
    "    frame = net.show(f'{name}.html', notebook=True, local=True)\n",
    "    display(frame)\n",
    "    \n",
    "def render_graph_sample(graph, target, k=2):        \n",
    "    sampler = KSampler(graph, target, k=k)\n",
    "    sample_graph, sample_target, sample_mapper = sampler.sample_as_graph(kc=True)\n",
    "    render_graph(sample_graph, sample_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e3aa5-958f-4e5c-94e0-7169dd9c9e1b",
   "metadata": {},
   "source": [
    "Below is the true output, with up to four different colors representing the classification of a facebook page, with edges representing mutual likes and nodes representing individual pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f173f9d3-adda-4dba-b60d-281a27d4e2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering sample by label...\n",
      "by_label.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400px\"\n",
       "            height=\"400px\"\n",
       "            src=\"by_label.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f57d0464eb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Rendering sample by label...\")\n",
    "render_graph(dataset.sample_graph, dataset.target, name=\"by_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a65e0fd-961b-48c6-b163-8f9ef45a0a21",
   "metadata": {},
   "source": [
    "Below is an output representing model accuracy on the sample, with green nodes representing correct predictions and red nodes representing incorrect predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70bba497-69ae-44df-b07e-f9ab48c9582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering sample by prediction...\n",
      "by_pred.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400px\"\n",
       "            height=\"400px\"\n",
       "            src=\"by_pred.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f590c313e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Rendering sample by prediction...\")\n",
    "render_graph(dataset.sample_graph, y_diff, name=\"by_pred\", kc=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
